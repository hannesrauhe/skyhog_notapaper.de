<article>
    
<h2>Get the feeling for data-intensive problems</h2>

    <div class="short">Before you consider porting an algorithm to the GPU you should check its
        characteristics. You may have often heard that it should be calculation/computation-intensive.
        In this post I try to describe what that means and be more specific. On
        the example of a simple re-encoding algorithm I show that this is not the
        whole truth. Data intensive algorithms can benefit from the high bandwidth
        of a GPU's RAM, but only if they fulfil certain conditions: they should
        support streaming and work in a massively parallel fashion.</div>
    <div class="tags">data-intensive streaming</div>
    <div class="date">2013-04-10</div>
    
<h3>Introduction</h3>

    <p>Although GPUs have more computational power and a higher bandwidth, only
        a subset of computations can benefit from these advantages. (I'm talking
        about dedicated graphic cards with own memory in this post - integrated
        GPUs are a different topic - I've seen some nice papers on the way discussing
        these in a DBMS context.) There are certain algorithm criteria that indicate
        a performance gain when using the GPU.</p>
    <ol>
        <li>the algorithm works in parallel with little or no overhead on the CPU</li>
        <li>the algorithm works massively parallel in a SIMD fashion</li>
        <li>processing the data on the CPU takes longer than transferring it to the
            GPU</li>
        <li>the data processed by the algorithm fits into the GPU's memory</li>
        <li>transfer to the GPU and actual computation can overlap</li>
    </ol>
    <p>These five point can be categorized: to adapt your algorithm to the GPU's
        architecture, the first two points are mandatory. The three other points
        basically deal with the bandwidth-bottleneck of an external Coprocessor.
        <br />I differentiate between the first two points, because on the one hand
        there are algorithms that are inherently sequential. Often they consist
        of a tight loop with thousands of iterations where each iteration depends
        on the previous one. A parallel implementation (even on the CPU) needs
        a totally different approach. On the other hand there are algorithms where
        a parallel implementation on the CPU is already available and looks promising.
        Often you can just partition the workload, process it with a number of
        different threads and merge the intermediate results in the end. If the
        merge overhead is comparatively small, e.g., it just involves copying the
        intermediate results to the right positions, the algorithm typically sclaes
        well with the number of CPU cores (until it is limited by the memory bandwidth).
        However, this type of algorithm usually does not scale well when more than
        a hundred partitions are used, because the merge overhead grows with the
        number of partitions. An alogrithm for the GPU needs to work with thousands
        of threads and in a SIMD fashion. One of the aspects of SIMD is that branching
        should be avoided. Every thread should calculate the same instruction for
        a different memory segment. I don't want to stretch this too much at this
        point, but remind you that there is a difference between a parallel and
        a massively parallel algorithm.
        <br />Point three is plain and simple but often forgotten. The only chance for
        an algorithm where data is processed faster than it is transferred, is
        that GPU and CPU may be able to split the work. But in all cases I can
        think of, such an algorithm is memory bound; so transferring the data to
        the GPU will slow down the memory access for the CPU as well. Point four
        does not mean that data bigger than the GPUs memory cannot be processed
        efficiently (although it may be a first sign...). But your algorithm should
        be able to transfer a partition of the data, process it and proceed to
        the next one. If you repeatedly need to transfer the same data back and
        forth because the free memory is needed in between, there is a good chance
        that there will be no benefit in using the GPU (or any other external Coprocessor).
        If, however, the data can be processed while it is transferred (point five,
        also called streaming), the bandwidth is the only limit. So even if the
        CPU is just slightly slower than the PCIe bus, there is a chance that the
        whole process is faster on the GPU. *streamable - data processed part for
        part - parts of the result are there early</p>
    
<h3>Experiment</h3>

    <p>We take a look at a very simple algorithm that is used in most column
        stores I know, e.g., SAP HANA. Every column is dictionary-encoded, i.e.,
        every distinct value of one column is stored in a sorted dictionary and
        the values in the column are references to the entries in the dictionary.
        Updates to the column are usually buffered and at some point integrated.
        This integration process (in HANA this is called Delta Merge) rebuilds
        the dictionary in a first step and updates the references in a second.
        We concentrate on this second step. Usually in the first step a mapping
        from the old reference to the new reference is created. Since references
        are just integers, the mapping is an array, where the new reference is
        stored at the position of the old one. Therefore, the algorithm just goes
        through every vector of references, and replaces each with the value that
        can be directly accessed in the map:</p>
<pre>
<code>
//src and dest can point to the same address
void recode_cpu(int *dest, const int* src, const int *map, size_t size) {
    for(uint idx=0;idx<size;++idx) {
        dest[idx] = map[src[idx]];
    }
}
</code>
</pre>

    <p>
<b>Implementation</b>

        <br />
        <br />The sequential CPU implementation is the one above. The second approach
        uses OpenMP to execute the loop in parallel.
        <br />Since we want to use streaming, a bit more effort is necessary for GPU
        execution. The <span>4-way-concurrency pattern</span> described <a href="http://developer.download.nvidia.com/CUDA/training/StreamsAndConcurrencyWebinar.pdf">here</a> is
        exactely what we need. At first we copy the map to the GPU. After that
        processing and streaming of the reference vector overlaps. Streaming needs
        pinned memory, so we allocate three pinned buffers in the CPU's RAM and
        in the GPU's RAM (size is 128kb - experiment with the size if you want).
        Now we do four things at once:</p>
    <ul>
        <li>the CPU copies references to the first buffer</li>
        <li>the GPU's copy engine copies references from a second buffer to the GPU's
            memory (first buffer there)</li>
        <li>the GPU processes the references in the second buffer</li>
        <li>the GPU's second copy engine copies the already processed elements from
            the third buffer on the GPU to the third buffer in main memory</li>
    </ul>
    <p>I allocated a big third buffer on the CPU to store everything there, otherwise
        the data has to be copied by the CPU in parallel to the target memory as
        a fifth step. GPU and PCIe bus provide enough redundant elements to do
        this all at once without loosing performance.</p>
    <p> <b>
      Hardware
     </b>

        <br>Our test machine is a Linux workstation equipped with two Xeon E5-2690
        hexa core CPUs and a NVidia Tesla K10 with 8 GB of RAM. The GPUs are connected
        via PCI-Express version 3 with a peak bandwidth of 11.2 GB/s. We measured
        the run-time for the operation with different data structures executed
        with one thread on the CPU, 32 threads on the CPU and on the GPU.</p>
    <object
    data="art4_fig_recoding.svg" type="image/svg+xml">Figure for recoding</object>
        <div class="references"></div>
        <div class="additional">The source code can be found here: <a href="https://github.com/hannesrauhe/notapaper_recode">https://github.com/hannesrauhe/notapaper_recode</a>

        </div>
</article>