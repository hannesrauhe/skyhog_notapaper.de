<article>
    
<h2>PGStrom for Postgress</h2>

    <div class="short"></div>
    <div class="tags">linear algebra</div>
    <div class="date">2013-04-28</div>
    
<h3>Introduction</h3>

    <p>A lot of colleagues mailed me the link to <a href="http://wiki.postgresql.org/wiki/PGStrom">PGStrom</a>, because they knew that this is in my domain and some of them also knew that 10x speedups and more by using a GPU in my field of work usually makes me skeptical. Let me summarize what PGStrom does on the GPU: it executes statements in the form of<br />
<code>SELECT * FROM table WHERE &lt;complex formula&gt;</code>< br/>
...on the GPU. This is not a bad idea at all. If you take a look at my <a href="article4.html">last blog post</a>, you can see that we have easily 5 checks out of 6. The algorithm can be processed in a SIMD fashion, streamed without overhead and we do not need to transfer data more than once (a.k.a. "fits into GPU memory"). However, the most important check is missing. I am not sure if the transfer to the GPU is faster than the computation on the CPU.<br />
Now the part, where I'm skeptical: I don't think that it is appropriate to compare Postgress against an optimized implementation on the GPU. They claim that they process 10 Mio records in 7 seconds with Postgress. 10 Mio always sounds like a huge number. But the example statement processes only two columns with floating point numbers. Assuming these numbers are 32 bit-float types, the CPU processes only 76 MB -- in 7 seconds! Remember that one CPU thread is capable of transferring around 4 GB of data per second!
</p>
    
<h3>Experiment and Implementation</h3>

    <p>In my opinion one way of a fair comparison is to hard-code this query in C and in CUDA and compare the run-time. This SQL statement</p>
<pre>
<code>
SELECT COUNT(*) FROM t1 WHERE sqrt((x-25.6)^2 + (y-12.8)^2) < 15;
</code>
</pre>
<p>becomes the following C code:</p>
<pre>
<code>
for(int i=0; i<(int)t_size; i++) {
  result += (sqrt( pow((x[i]-25.6),2) + pow((y[i]-12.8),2)) < 15);
}
</code>
</pre>

    <p>This can easily be parallelized with openmp. The GPU implementation is very similar to the one from the last post. We copy and process data on CPU and GPU in parallel. This requires 5 lines of work:</p>
<ol>
<li>copy data from the columns to a buffer in main memory (this must be pinned memory to allow async. transfers)</li>
<li>copy data from the main memory buffer to the GPU memory buffer</li>
<li>process/filter data in the GPU memory buffer by executing the kernel on the GPU</li>
<li>copy the result of the kernel (a bitvector) from GPU to CPU</li>
<li>count the 1-bits on the CPU and add this to the final result</li>
</ol>

<p>
The buffers hold 64*1024 floating point numbers each.
You can adjust this by setting the following in pgstrom_test.h: 
</p>

<pre>
<code>
static const int STREAM_BUF_SIZE = 64*1024; //in number of floats
#define NUMBER_OF_THREADS 512
</code>
</pre>

<h4>Hardware</h4>

    <p>Our test machine is a Linux workstation equipped with two Xeon E5-2690
        hexa core CPUs and a NVidia Tesla K20 with 8 GB of RAM. The GPUs are connected
        via PCI-Express version 3 with a peak bandwidth of 11.2 GB/s.</p>
    
<h4>Results</h4>

    <p>
    </p>
    
<h3>Conclusion</h3>

    <p></p>
        <div class="references">

        </div>
        <div class="additional">


        </div>
</article>